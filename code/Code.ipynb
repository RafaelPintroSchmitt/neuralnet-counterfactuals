{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABPLlxYA8PjF"
      },
      "source": [
        "# Electoral geography: predicting the universe of city-level vote-shares with a sample of cities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2BGXMsc8PjJ"
      },
      "source": [
        "Outline: the objectives of this exercise are:\n",
        "\n",
        "(1) to predict changes in the vote-share of the workers party in Brazil (PT) for every municipality in Brazil, using only a subset of such municipalities. I will train a Graph Neural Network to make predictions.\n",
        "\n",
        "(2) given some treatment affecting a subset of municipalities (treated), use the model to predict changes in the vote-share of the treated group using only the control group (non-treated). I will train the model in the years before treatment, and I will use the model to predict the counterfactual outcome of the treated using the control group, before and after treatment. The model bias will be estimated as the difference between the predicted and the actual outcome of the treated group. If the model is roughly unbiased before treatment, then the difference between the predicted and the actual outcome of the treated group after treatment will be a measure of the average treatment effect (ATE = -bias)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLloMpAj8PjK"
      },
      "source": [
        "First, import the necessary libraries and load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC5lZQSdE4Wx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(r\"C:\\Users\\rafap\\Desktop\\RA2023 - personal files\\neuralnet-counterfactuals\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2wttAnb8PjL",
        "outputId": "f43687af-861b-4d83-dce6-a15e59ffe54a"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data, Batch\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch_geometric.utils\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.manifold import TSNE\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv # [^1^][1]\n",
        "\n",
        "# Load the dta datasets into a pandas dataframe\n",
        "df2006 = pd.read_stata(rf\"data/brazilian_municipalities2006.dta\")\n",
        "df2010 = pd.read_stata(rf\"data/brazilian_municipalities2010.dta\")\n",
        "df2014 = pd.read_stata(rf\"data/brazilian_municipalities2014.dta\")\n",
        "df2018 = pd.read_stata(rf\"data/brazilian_municipalities2018.dta\")\n",
        "df2022 = pd.read_stata(rf\"data/brazilian_municipalities2022.dta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnCt2Fpm-3tC"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "device_cpu = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOB2xo2p8PjM"
      },
      "source": [
        "Now, I create a list containing the graphs of each election. Each graph is a tensor_geometric Data object, which contains the adjacency matrix and the features of each node. The graph structure is defined by linking each city to it's 10 closest peers (in terms of geographical distance).\n",
        "\n",
        "Roughly, I take the datasets for the years 2010 through 2022 and I define the output vector (Y) as the difference in vote-share in the runoff for the PT (workers party) between the current and last election. The X matrix, containing covariates, includes city-level covariates extracted from the Brazilian 2010 census. Note that it also includes the output vector (difference in vote-share). The goal here is to train the network by (1) dropping some cities in the training set arbitralily, and (2) predicting the vote-share of the PT in those cities through convolutions/attention layers. That is, I leverage \"local\" voting patterns to extrapolate voting behavior to the omitted-outcome cities. The network should thus be learning an underlying geographical contagion/distribution process of voting behavior. In evaluating the model, I will only use cities out of the training set. In particular, I will drop \"diff\" for half the cities in the test set and predict their vote-share based on the other half (see the definition of Z).\n",
        "\n",
        "I also define treatment variables here - i.e. I drop the \"labels\" or outcomes of the treated cities, and will use the non-dropped controls to predict the counterfactuals for the treated cities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyZhlP908PjM",
        "outputId": "85907551-dfa7-4000-a05d-b665e6984ff1"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "treatment_list = []\n",
        "treatment_mask = []\n",
        "Z_list = []\n",
        "Zmask_list = []\n",
        "#loop over years\n",
        "for df in [df2010, df2014, df2018, df2022]:\n",
        "    # encode the cateogrical variables as integers\n",
        "    df[\"regiao\"] = df[\"regiao\"].astype(\"category\").cat.codes\n",
        "    df[\"sigla_uf\"] = df[\"sigla_uf\"].astype(\"category\").cat.codes\n",
        "\n",
        "    #drop missing values\n",
        "    df = df[~np.isnan(df).any(axis=1)]\n",
        "\n",
        "    #create difference from previous election to current election\n",
        "    df[\"diff\"] = df[\"pt_share\"] - df[\"l4pt_share\"]\n",
        "    Y = df.loc[:, [\"diff\"]]\n",
        "    #divide Y in 100 quantiles\n",
        "    Y = pd.qcut(Y[\"diff\"], q=100, labels=False)\n",
        "    Y = Y.to_numpy()\n",
        "\n",
        "    X = df.loc[:, [\"diff\", \"l4pt_share\", \"populacao\", \"evangelico\", \"urban\", \"radio\", \"televisao\", \"idade\", \"alfabetizado\", \"rend_total\", \"area\", \"density\", \"white\", \"nasceu_mun\", \"horas_trabprin\", \"filhos_nasc_vivos\", \"high_school\", \"bachelor\", \"vive_conjuge\", \"sexo\", \"high\"]]\n",
        "    #save the collumn high in a pd series\n",
        "    high = X[\"high\"]\n",
        "    #normalize the data\n",
        "    X = (X - X.mean()) / X.std()\n",
        "    #add the collumn high to the normalized data\n",
        "    X[\"high\"] = high\n",
        "\n",
        "    # Create treat, which is a copy of df with the values of diff set to 0 if high (my treatment variable) is 1\n",
        "    # This is to evaluate the effect of treatment in 2022\n",
        "    treat = X.copy().to_numpy()\n",
        "    treat[:, 0] = treat[:, 0] * (1 - treat[:, 20])\n",
        "    #drop collumn high from treat\n",
        "    treat = treat[:, :-1]\n",
        "    # Create a tensor for treat\n",
        "    treat = torch.tensor(treat, dtype=torch.float).to(device)\n",
        "    # Create a boolean taking values 1 if treat is 0 and 1 otherwise (use sourceTensor.clone().detach())\n",
        "    treat_mask = treat[:, 0].clone().detach()\n",
        "    treat_mask[treat_mask != 0] = 1\n",
        "    treat_mask = torch.logical_not(treat_mask).to(device)\n",
        "    treatment_list.append(treat)\n",
        "    treatment_mask.append(treat_mask)\n",
        "\n",
        "    #drop the collumn high\n",
        "    X = X.drop(columns=[\"high\"])\n",
        "\n",
        "    # Create Z, which is a copy of X with a random 50% of the values in diff set to 0\n",
        "    # This is for testing the model at the end\n",
        "    Z = X.copy().to_numpy()\n",
        "    Z[:, 0] = np.random.choice([0, 1], size=Z.shape[0], p=[0.5, 0.5]) * Z[:, 0]\n",
        "    # Create a tensor for Z\n",
        "    Z = torch.tensor(Z, dtype=torch.float).to(device)\n",
        "    # Create a boolean taking values 1 if Z is 0 and 1 otherwise (use sourceTensor.clone().detach())\n",
        "    Z_mask = Z[:, 0].clone().detach()\n",
        "    Z_mask[Z_mask != 0] = 1\n",
        "    Z_mask = torch.logical_not(Z_mask).to(device)\n",
        "    Z_list.append(Z)\n",
        "    Zmask_list.append(Z_mask)\n",
        "\n",
        "    # Extract the coordinates as a numpy array\n",
        "    coords = df[[\"_Y0\", \"_X0\"]].to_numpy()\n",
        "\n",
        "    # Alternatively, use a k-nearest neighbors algorithm to find the 50 nearest neighbors for each city\n",
        "    nn = NearestNeighbors(n_neighbors=10, metric=\"euclidean\")\n",
        "    nn.fit(coords)\n",
        "    dist_sparse = nn.kneighbors_graph(mode=\"distance\")\n",
        "\n",
        "    # Convert the sparse matrix to edge index and edge weight tensors\n",
        "    edge_index, edge_attr = torch_geometric.utils.convert.from_scipy_sparse_matrix(dist_sparse)\n",
        "\n",
        "    # Make X and Y tensors\n",
        "    X = torch.tensor(X.to_numpy(), dtype=torch.float)\n",
        "    Y = torch.tensor(Y, dtype=torch.float)\n",
        "    data = Data(x=X, y=Y, edge_attr=edge_attr, edge_index=edge_index)\n",
        "\n",
        "    # Get the number of cities\n",
        "    num_cities = data.num_nodes\n",
        "\n",
        "    # Randomly select 2500 cities for training\n",
        "    perm = torch.randperm(num_cities)\n",
        "    train_idx = perm[:2500]\n",
        "\n",
        "    # Split the remaining cities into validation and test sets\n",
        "    val_idx, test_idx = torch.split(perm[2500:], num_cities // 2)\n",
        "\n",
        "    # Create boolean masks for each set\n",
        "    train_mask = torch.zeros(num_cities, dtype=torch.bool)\n",
        "    train_mask = train_mask.scatter(0, train_idx, 1)\n",
        "\n",
        "    val_mask = torch.zeros(num_cities, dtype=torch.bool)\n",
        "    val_mask = val_mask.scatter(0, val_idx, 1)\n",
        "\n",
        "    test_mask = torch.zeros(num_cities, dtype=torch.bool)\n",
        "    test_mask = test_mask.scatter(0, test_idx, 1)\n",
        "\n",
        "    # Add the masks as attributes to the data object\n",
        "    data.train_mask = train_mask\n",
        "    data.val_mask = val_mask\n",
        "    data.test_mask = test_mask\n",
        "\n",
        "    # Convert the labels to long tensors\n",
        "    data.y = data.y.long()\n",
        "\n",
        "    data.validate(raise_on_error=True)\n",
        "\n",
        "    in_features = X.shape[1]\n",
        "    #define out_features as the number of quantiles in Y, as a function of Y\n",
        "    out_features = len(np.unique(Y))\n",
        "\n",
        "    #put in gpu\n",
        "    data.to(device)\n",
        "\n",
        "    #append to data_list\n",
        "    data_list.append(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc-Snw448PjN"
      },
      "source": [
        "We now have the graphs for each year stored in data_list. We can access some graph information as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZRlpujU8PjN",
        "outputId": "6da9c1f7-36ff-42cd-ff9d-34698ae0f636"
      },
      "outputs": [],
      "source": [
        "print(data_list[0])\n",
        "print('==============================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrE9JMNZ8PjN"
      },
      "source": [
        "To test whether the predictions also generalize across years, I take 2022 (and possibly 2018) away from the training set. I'll then apply the model to 2022 and check the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGlIbPjD8PjO",
        "outputId": "f81dfb81-c2ad-4068-916c-3974678892e0"
      },
      "outputs": [],
      "source": [
        "# store all the elections\n",
        "data2010 = data_list[0]\n",
        "data2014 = data_list[1]\n",
        "data2018 = data_list[2]\n",
        "\n",
        "# store the last item of data_list to data2022\n",
        "data2022 = data_list[-1]\n",
        "data_list.pop(-1)\n",
        "#data_list.pop(-1) #if want to train without 2018\n",
        "data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a19DX_p8PjO"
      },
      "source": [
        "I now turn to defining the model. I will use a Graph Attention Network (GAT). I have a dropout layer, which omits the information of some nodes in each layer. I then have two GAT layers, with a RELU activation function in between, which are the core of the model. Finally, I have a linear layer, which outputs the predicted vote-share for the PT in each city. The model is defined as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWVsHTdG8PjO",
        "outputId": "d2badf25-7963-422a-ba2a-04af580e9a21"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import random\n",
        "j = random.randint(0, 1000)\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(j)\n",
        "        self.conv1 = GATConv(in_features, in_features, heads=2)\n",
        "        self.conv2 = GATConv(in_features*2, in_features, heads=2)\n",
        "        self.lin1 = Linear(in_features*2, out_features)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        #make drops start at 0.9 and decrease by 0.05 every 10 epochs, until it reaches 0.5\n",
        "        drops = 0.9 - (0.05 * (epoch // 10))\n",
        "        if drops < 0.5:\n",
        "            drops = 0.5\n",
        "        x = F.dropout(x, p=drops, training=self.training)\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index, edge_attr)\n",
        "        x = self.lin1(x)\n",
        "        return x\n",
        "\n",
        "model = GAT()\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwspCSL-8PjP"
      },
      "source": [
        "The training of the model is simple: for every epoch I actually take one optimization step for each election included in the training. By alternating the elections at every training step (instead of training on each election sequentially), I hope to capture a more robust relationship underlying electoral geographical distributions. Note that I start by dropping 90% of the nodes in the dropout layer, going down to 50% as the training progresses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eafnzCYa8PjP",
        "outputId": "ff48df51-3732-40a9-ddc3-d6b180222f6b"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "def train():\n",
        "    #loop over items of datalist\n",
        "    for data in data_list:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "        out = model(data.x, data.edge_index, data.edge_attr)  # Perform a single forward pass.\n",
        "        # Compute the loss solely based on the training nodes.\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "    return loss\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "    drops = 0.9\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WImcYUWM8PjP"
      },
      "source": [
        "Testing is defined as follows. There are 3 functions. The main one (test(year)), is used to test the model's predictions for the treatment groups defined above. Unexpected biases in the predictions of the model are interpreted as the effect of treatment (since we are comparing with the models \"counterfactual\" predictions).\n",
        "\n",
        "The other two are simple test functions, which evaluate the models capabilities to predict vote-shares of a sample of randomly selected cities given the remaining ones' vote-shares in 2022. More precisely: Z is defined above by taking data2022 and setting half of the values of diff to 0. Z_mask is a boolean vector taking value 1 if diff was set to 0 in a particular observation. Here, I evaluate the models ability to predict diff for those cities that had their diff set to 0.\n",
        "\n",
        "test_testset() evaluates the model's fit for the test set, whereas test_trainset() evaluates the model's fit for the training set (the latter is mainly to diagnose overfitting in case the accuracy in the training set is much larger than in the test set.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "rWNQPgSO8PjP",
        "outputId": "7f1d2345-abb8-417d-f03d-2f5e568b7618"
      },
      "outputs": [],
      "source": [
        "def test(year):\n",
        "      model.eval()\n",
        "      #test on each dataset in datalist\n",
        "      data = data_list\n",
        "      #data.append(data2018) #if trained without 2018\n",
        "      data.append(data2022)\n",
        "      data = data[int((year+2)/4-503)] #the weird integer is just the list index (0,1,2,3) from the year list (2010, 2014, 2018, 2022)\n",
        "      out = model(treatment_list[int((year+2)/4-503)], data.edge_index, data.edge_attr)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      #note: no need to use treatment_mask list since the selection across years is constant (what changes is the dataset with diff)\n",
        "      mask = treat_mask*data.test_mask #add *data.test_mask if year is in in the training set (so we exclude the training cities for each year)\n",
        "      test_correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
        "      # calculate the mean squared error, make input a float tensor\n",
        "      mse = torch.mean((pred[mask].float() - data.y[mask].float())**2)\n",
        "      # bias\n",
        "      bias = torch.mean((pred[mask].float() - data.y[mask].float()))\n",
        "      # generate a tensor with the values of data.y[mask] but randomly shuffled\n",
        "      shuffled = data.y[mask].float().clone()\n",
        "      shuffled = shuffled[torch.randperm(len(shuffled))]\n",
        "      #mse with suffled data\n",
        "      mse_shuffled = torch.mean((shuffled - data.y[mask].float())**2)\n",
        "      #fake rsquared\n",
        "      r2 = 1 - mse/mse_shuffled\n",
        "      #plot the two histograms together, with transparency and different colors\n",
        "      plt.hist(pred[mask].cpu().float() - data.y[mask].cpu().float(), alpha=0.5, label='Predictions')\n",
        "      plt.hist(shuffled.cpu() - data.y[mask].cpu().float(), alpha=0.5, label='Shuffled')\n",
        "      plt.legend(loc='upper right')\n",
        "      plt.show()\n",
        "      return test_acc, pred, mse, bias, r2, out\n",
        "\n",
        "#Test on the test data\n",
        "def test_testset(year):\n",
        "      model.eval()\n",
        "      #test on each dataset in datalist\n",
        "      data = data_list\n",
        "      #data.append(data2018) #if trained without 2018\n",
        "      data.append(data2022)\n",
        "      data = data[int((year+2)/4-503)] #the weird integer is just the list index (0,1,2,3) from the year list (2010, 2014, 2018, 2022)\n",
        "      out = model(Z_list[int((year+2)/4-503)], data.edge_index, data.edge_attr)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      mask = Zmask_list[int((year+2)/4-503)]*data.test_mask #add *data.test_mask if year is in in the training set (so we exclude the training cities for each year)\n",
        "      test_correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
        "      # calculate the mean squared error, make input a float tensor\n",
        "      mse = torch.mean((pred[mask].float() - data.y[mask].float())**2)\n",
        "      # bias\n",
        "      bias = torch.mean((pred[mask].float() - data.y[mask].float()))\n",
        "      # generate a tensor with the values of data.y[mask] but randomly shuffled\n",
        "      shuffled = data.y[mask].float().clone()\n",
        "      shuffled = shuffled[torch.randperm(len(shuffled))]\n",
        "      #mse with suffled data\n",
        "      mse_shuffled = torch.mean((shuffled - data.y[mask].float())**2)\n",
        "      #fake rsquared\n",
        "      r2 = 1 - mse/mse_shuffled\n",
        "\n",
        "      #plot the two histograms together, with transparency and different colors\n",
        "      plt.hist(pred[mask].cpu().float() - data.y[mask].cpu().float(), alpha=0.5, label='Predictions')\n",
        "      plt.hist(shuffled.cpu() - data.y[mask].cpu().float(), alpha=0.5, label='Shuffled')\n",
        "      plt.legend(loc='upper right')\n",
        "      #save plot\n",
        "      plt.savefig(rf\"outputs/performance.pdf\")\n",
        "      return test_acc, pred, mse, bias, r2, out\n",
        "\n",
        "#test on the training data\n",
        "def test_trainset():\n",
        "        model.eval()\n",
        "        data = data2022\n",
        "        out = model(Z, data.edge_index, data.edge_attr)\n",
        "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "        mask = Z_mask*data.train_mask #add *data.train_mask if data2022 in the training set\n",
        "        test_correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
        "        test_acc = int(test_correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
        "        return test_acc\n",
        "\n",
        "\n",
        "test_acc, pred, mse, bias, r2, out = test(2022)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "print(f'MSE: {mse:.4f}')\n",
        "print(f'Bias: {bias:.4f}')\n",
        "print(f'Fake R2: {r2:.4f}')\n",
        "train_acc = test_trainset()\n",
        "print(\"-------------------------\")\n",
        "print(f'Training Accuracy: {train_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROZsEtUg8PjQ"
      },
      "source": [
        "Now I train the model 100 times. For each training, I save the model's average bias in the treatment group predictions. I'll then plot the bias distribution for each year.\n",
        "\n",
        "Note that if I use test() here, I will be storing the average bias in the model's prediction to my treatment group. As mentioned before, this allows me to estimate a treatment effect under the assumption that the model's predictions are a good counterfactual in the absence of treatment. I will then plot the distribution of the treatment effect estimates for each year.\n",
        "\n",
        "We can easily do a placebo analysis, by instead using test_testset(), which instead just amounts to storing bias values for a random selection of cities at every run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzpM61fB8PjQ",
        "outputId": "5a4434e3-01c3-405d-990e-620cab148a04"
      },
      "outputs": [],
      "source": [
        "#for some reason running the code calling another file is MUCH faster.\n",
        "bias_list = []\n",
        "for loop in range(100):\n",
        "    #toberun is just a copy of the code above (not including this and what follows).\n",
        "    with open(r\"code\\toberun.py\") as f:\n",
        "        exec(f.read())\n",
        "    bias = []\n",
        "    for year in range(2010, 2026, 4):\n",
        "        #use test_testset if want to evaluate on \"fake\" treatment (i.e. model performance across years)\n",
        "        test_acc_temp, pred, mse_temp, bias_temp, r2_temp, out = test(year)\n",
        "        bias.append(bias_temp)\n",
        "    bias_list.append(bias)\n",
        "    print(loop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djE4ew6G8PjQ"
      },
      "source": [
        "What follows are just manipulations of the data to plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ6hsMHM8PjQ"
      },
      "outputs": [],
      "source": [
        "#make bias_list a numpy array\n",
        "bias_list = [[x.cpu().item() for x in y] for y in bias_list]\n",
        "bias_list = np.array(bias_list)\n",
        "\n",
        "#calculate the mean of each column\n",
        "bias_mean = np.mean(bias_list, axis=0)\n",
        "\n",
        "#save bias_list as a csv file\n",
        "np.savetxt(rf\"outputs/bias_list.csv\", bias_list, delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg-wn7BY8PjR"
      },
      "outputs": [],
      "source": [
        "# gen long_bias, which is bias_list but in panel data form. The resulting dataframe has 2 collumns: year and bias\n",
        "long_bias = pd.DataFrame(bias_list)\n",
        "\n",
        "#training on 2010-2018\n",
        "#long_bias = pd.read_csv(rf\"{path}\\outputs\\results_train_2010_2018.csv\", header=None)\n",
        "long_bias2 = pd.read_csv(rf\"outputs/results_2010_2018_random.csv\", header=None)\n",
        "\n",
        "#training on 2010-2014\n",
        "#long_bias = pd.read_csv(r'C:\\Users\\rafap\\Documents\\Masters\\fourth semester\\MachineLearning\\GNN project\\results_train_2010_2014.csv', header=None)\n",
        "#long_bias2 = pd.read_csv(r'C:\\Users\\rafap\\Documents\\Masters\\fourth semester\\MachineLearning\\GNN project\\results_2010_2014_random.csv', header=None)\n",
        "\n",
        "#get bias_list in from the saved csv file\n",
        "long_bias = long_bias.stack().reset_index()\n",
        "\n",
        "#make level_1 be called year and set 0 as 2010, 1 as 2014, etc\n",
        "long_bias = long_bias.rename(columns={'level_1': 'year'})\n",
        "long_bias['year'] = long_bias['year'].replace([0, 1, 2, 3], [2010, 2014, 2018, 2022])\n",
        "#rename collumn 0 as bias\n",
        "long_bias = long_bias.rename(columns={0: 'bias'})\n",
        "#drop everything but bias and year\n",
        "long_bias = long_bias.drop(columns=['level_0'])\n",
        "\n",
        "#do the same for long_bias2\n",
        "long_bias2 = long_bias2.stack().reset_index()\n",
        "long_bias2 = long_bias2.rename(columns={'level_1': 'year'})\n",
        "long_bias2['year'] = long_bias2['year'].replace([0, 1, 2, 3], [2010, 2014, 2018, 2022])\n",
        "long_bias2 = long_bias2.rename(columns={0: 'bias'})\n",
        "long_bias2 = long_bias2.drop(columns=['level_0'])\n",
        "#replace 2010 with 2010_1, 2014 with 2014_1, etc\n",
        "long_bias2['year'] = long_bias2['year'].replace([2010, 2014, 2018, 2022], [2011, 2015, 2019, 2023])\n",
        "\n",
        "#concatenate long_bias and long_bias2\n",
        "long_bias = pd.concat([long_bias, long_bias2], ignore_index=True)\n",
        "\n",
        "#create var random, which is 1 if the year is in the list 2010.1, 2014.1 etc and 0 otherwise\n",
        "long_bias['random'] = np.where(long_bias['year'].isin([2011, 2015, 2019, 2023]), 1, 0)\n",
        "#make random a categorical variable\n",
        "long_bias['random'] = long_bias['random'].astype('category')\n",
        "#make year a categorical variable\n",
        "long_bias['year'] = long_bias['year'].astype('category')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD6ISLv38PjR",
        "outputId": "2e9f7eaa-f0c9-4d9c-be7e-33477a6efc4e"
      },
      "outputs": [],
      "source": [
        "from plotnine import *\n",
        "\n",
        "p = (\n",
        "    # Plot violin plots for each year\n",
        "    ggplot(long_bias, aes(x='factor(year)', y='bias', fill=\"random\")) +\n",
        "    geom_violin(scale='width', trim=False, alpha=0.2) +\n",
        "    # Plot the mean bias for each year\n",
        "    geom_point(aes(x='factor(year)', y='bias'), stat='summary', fun_y=np.mean, size=2) +\n",
        "    # Plot the 95% confidence interval for the mean bias for each year\n",
        "    geom_errorbar(aes(x='factor(year)', ymin='bias', ymax='bias'), stat='summary', fun_ymin=lambda x: np.mean(x)-1.96*np.std(x), fun_ymax=lambda x: np.mean(x)+1.96*np.std(x), width=0.2) +\n",
        "    # Plot a horizontal dashed line at 0\n",
        "    geom_hline(yintercept=0, linetype='dashed')\n",
        "    #x axis label\n",
        "    + xlab('Year')\n",
        "    #y axis label\n",
        "    + ylab('Bias')\n",
        "    #group ticks two by two and label them 2010, 2014, etc\n",
        "    + scale_x_discrete(breaks=[2010, 2014, 2018, 2022], labels=['2010', '2014', '2018', '2022'])\n",
        "     #do not show label legend instead of \"random\"\n",
        "    + labs(fill='')\n",
        "    #change legend position\n",
        "    + theme(legend_position='bottom')\n",
        "    #change style of the legend\n",
        "    + theme(legend_key=element_rect(fill='white', colour='white'))\n",
        "    #change the 0 and 1 in the legend to \"Treatment\" and \"Placebo Treatment\"\n",
        "    + scale_fill_discrete(labels=['Treatment', 'Placebo'])\n",
        "    #make the background of the plot white\n",
        "    + theme_bw()\n",
        ")\n",
        "\n",
        "#2010_2018 version\n",
        "p.save(rf\"outputs/2010_2018.pdf\", width=6, height=4)\n",
        "#2010_2014 version\n",
        "#p.save(r'C:\\Users\\rafap\\Documents\\Masters\\fourth semester\\MachineLearning\\GNN project\\2010_2014.pdf', width=6, height=4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
